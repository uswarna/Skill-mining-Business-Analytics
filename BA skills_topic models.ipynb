{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Only India JD\n",
    "#    800 JDs after cleaning \n",
    "#    JDs from India\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import spacy \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1=pd.read_csv(\"indeed_in (1).csv\", sep=\",\")\n",
    "i2=pd.read_csv(\"indeed_in (2).csv\", sep=\",\")\n",
    "i3=pd.read_csv(\"indeed_in.csv\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2552 entries, 0 to 2551\n",
      "Data columns (total 8 columns):\n",
      "web-scraper-order        2552 non-null object\n",
      "web-scraper-start-url    2552 non-null object\n",
      "job_title                2552 non-null object\n",
      "job_title-href           2552 non-null object\n",
      "job_compay               638 non-null object\n",
      "job_location             638 non-null object\n",
      "job_description          638 non-null object\n",
      "title                    638 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 159.6+ KB\n"
     ]
    }
   ],
   "source": [
    "i1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_title', 'job_description'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=pd.concat([i1,i2,i3])\n",
    "new=new.drop(['web-scraper-order', 'web-scraper-start-url', 'job_title-href',\n",
    "       'title','job_compay', 'job_location'],axis=1)\n",
    "new.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-1252\n"
     ]
    }
   ],
   "source": [
    "import chardet    \n",
    "rawdata = open(\"ba_in_jobs.csv\", 'rb').read()\n",
    "result = chardet.detect(rawdata)\n",
    "charenc = result['encoding']\n",
    "print(charenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['job_title', 'job_description'], dtype='object')\n",
      "(495, 2)\n"
     ]
    }
   ],
   "source": [
    "i4=pd.read_csv(\"ba_in_jobs.csv\",sep=\",\",encoding = 'Windows-1252')\n",
    "i4=i4.drop(['company_name', 'job_location', 'ID'],axis=1)\n",
    "print(i4.columns)\n",
    "print(i4.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3703, 2)\n"
     ]
    }
   ],
   "source": [
    "df=pd.concat([new,i4])\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1297, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[pd.isnull(df.job_description)==False]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ba_con.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text']=df['job_description'].apply(lambda x:x.translate(str.maketrans(string.punctuation,' '*len(string.punctuation))))\n",
    "\n",
    "#Maketrans the first arguemenet is what needs to be replaced,the second arguement is what to be replaced with and the third arguement is the stuff u want to delete\n",
    "\n",
    "\n",
    "df['clean_text']=df['clean_text'].apply(lambda x:x.translate(str.maketrans(string.digits,' '*len(string.digits))))\n",
    "\n",
    "df['clean_text']=df['clean_text'].str.replace('\\n',' ')\n",
    "df['clean_text']=df['clean_text'].str.replace('\\t',' ')\n",
    "df['clean_text']=df['clean_text'].str.replace('\\r',' ')\n",
    "\n",
    "#df['clean_text']=df['clean_text'].str.lower()\n",
    "\n",
    "\n",
    "#df['clean_text'].apply(lambda s:re.sub( r\"([A-Z])\", r\" \\1\", s).split())#Splitting strings based on capital letters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "def split_upper(dump):\n",
    "    words = dump.split()\n",
    "    for word in words:\n",
    "        if len(word)>=10:\n",
    "            a.append((' ').join(re.sub( r\"([A-Z])\", r\" \\1\", word).split()))\n",
    "        else:\n",
    "            a.append(word)\n",
    "    return (' ').join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text']=df['clean_text'].apply(lambda x:split_upper(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_title', 'job_description', 'clean_text'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swarna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\swarna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\swarna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.chunk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nouns = [token for token, pos in pos_tag(word_tokenize(sentence)) if pos.startswith('N','A','V')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={\"Power BI\":\"PowerBI\",\"power BI\":\"PowerBI\",\"DAX\":\"PowerBI\",\"reporting\":\"Reporting_Analysis\",\n",
    "     \"hoc reporting\":\"Reporting_Analysis\",\"reporting analysis\":\"Reporting_Analysis\",\"data mining\":\"DataMining\",\n",
    "     \"data science\":\"DataScience\",\"Data Science\":\"DataScience\",\"data analysis\":\"DataAnalysis\",\n",
    "     \"machine learning\":\"MachineLearning\",\"Business Intelligence\":\"BusinessIntelligence\",\n",
    "     \"spreadsheets\":\"Excel\",\"Microsoft Office\":\"MicrosoftOffice\",\"business intelligence\":\"BusinessIntelligence\",\n",
    "     \"visualization\":\"visualisation\",\"Dashboarding\":\"Dashboard\"}\n",
    "\n",
    "def replace_all(text,**dic):\n",
    "\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "df['clean_text']=df['clean_text'].apply(lambda x:replace_all(x,**dic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#splitting df to avoid memory error\n",
    "msk = np.random.rand(len(df)) < 0.30\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324, 3) (324, 3) (389, 3) (260, 3)\n"
     ]
    }
   ],
   "source": [
    "df1, df2, df3,df4 = np.split(df.sample(frac=1), [int(.25*len(df)), int(.5*len(df)),int(0.8*len(df))])\n",
    "print(df1.shape,df2.shape,df3.shape,df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stop(x):\n",
    "     x = [w for w in x.split() if w not in set(stopwords)]  # remove stopwords\n",
    "    return ' '.join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[df2,df3,df4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                          | 0/324 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▌                                                                                 | 2/324 [00:00<00:29, 11.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▊                                                                                 | 3/324 [00:00<00:48,  6.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|█                                                                                 | 4/324 [00:00<00:56,  5.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|█▎                                                                                | 5/324 [00:01<01:41,  3.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|█▌                                                                                | 6/324 [00:01<02:10,  2.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|█▊                                                                                | 7/324 [00:02<02:17,  2.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|██                                                                                | 8/324 [00:02<02:11,  2.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|██▎                                                                               | 9/324 [00:03<01:47,  2.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|██▌                                                                              | 10/324 [00:03<02:34,  2.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|██▊                                                                              | 11/324 [00:04<02:52,  1.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|███▎                                                                             | 13/324 [00:05<02:36,  1.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|███▌                                                                             | 14/324 [00:05<02:30,  2.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|███▊                                                                             | 15/324 [00:06<02:24,  2.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|████                                                                             | 16/324 [00:06<02:52,  1.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|████▎                                                                            | 17/324 [00:07<02:43,  1.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|████▌                                                                            | 18/324 [00:08<03:06,  1.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|████▊                                                                            | 19/324 [00:09<03:30,  1.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|█████                                                                            | 20/324 [00:09<03:12,  1.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|█████▎                                                                           | 21/324 [00:10<03:17,  1.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|█████▌                                                                           | 22/324 [00:10<03:07,  1.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|█████▊                                                                           | 23/324 [00:11<02:50,  1.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██████                                                                           | 24/324 [00:11<02:58,  1.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████▎                                                                          | 25/324 [00:12<02:57,  1.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████▊                                                                          | 27/324 [00:13<02:28,  2.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███████                                                                          | 28/324 [00:13<02:55,  1.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███████▎                                                                         | 29/324 [00:14<02:23,  2.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███████▌                                                                         | 30/324 [00:14<02:51,  1.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███████▊                                                                         | 31/324 [00:15<02:23,  2.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████████                                                                         | 32/324 [00:15<02:04,  2.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████████▎                                                                        | 33/324 [00:15<01:50,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████████▌                                                                        | 34/324 [00:16<02:01,  2.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████████▊                                                                        | 35/324 [00:16<01:45,  2.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█████████                                                                        | 36/324 [00:17<02:02,  2.36it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████████▌                                                                       | 38/324 [00:17<01:43,  2.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████████▊                                                                       | 39/324 [00:18<01:57,  2.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|██████████▎                                                                      | 41/324 [00:18<01:44,  2.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|██████████▊                                                                      | 43/324 [00:18<01:17,  3.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|███████████                                                                      | 44/324 [00:18<01:15,  3.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|███████████▎                                                                     | 45/324 [00:19<02:00,  2.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|███████████▌                                                                     | 46/324 [00:20<02:06,  2.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|████████████                                                                     | 48/324 [00:20<01:53,  2.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|████████████▎                                                                    | 49/324 [00:21<02:11,  2.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|████████████▌                                                                    | 50/324 [00:22<02:33,  1.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█████████████                                                                    | 52/324 [00:23<02:20,  1.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█████████████▎                                                                   | 53/324 [00:23<01:49,  2.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█████████████▌                                                                   | 54/324 [00:23<01:25,  3.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█████████████▊                                                                   | 55/324 [00:23<01:18,  3.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████████████                                                                   | 56/324 [00:24<01:30,  2.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████████████▎                                                                  | 57/324 [00:24<01:37,  2.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████████████▌                                                                  | 58/324 [00:25<01:49,  2.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████████████▊                                                                  | 59/324 [00:25<01:25,  3.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████████████                                                                  | 60/324 [00:25<01:20,  3.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████████████▎                                                                 | 61/324 [00:25<01:28,  2.98it/s]"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-c6fa77f78f76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_stop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m                 \u001b[1;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m                 \u001b[1;31m# Close bar and return pandas calculation result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4042\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m                     \u001b[1;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-c6fa77f78f76>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(cell)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_stop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df2['text_stop']=df2['clean_text'].progress_apply(lambda cell:' '.join([item for item in cell.split() if item not in stop]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swarna\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/376 [00:00<?, ?it/s]\n",
      "  2%|█▌                                                                                | 7/376 [00:00<00:06, 54.28it/s]\n",
      "  3%|██▏                                                                              | 10/376 [00:00<00:09, 40.26it/s]\n",
      "  3%|██▊                                                                              | 13/376 [00:00<00:11, 32.45it/s]\n",
      "  4%|███▏                                                                             | 15/376 [00:00<00:13, 26.11it/s]\n",
      "  5%|███▋                                                                             | 17/376 [00:00<00:17, 21.06it/s]\n",
      "  5%|████                                                                             | 19/376 [00:00<00:20, 17.48it/s]\n",
      "  6%|████▌                                                                            | 21/376 [00:00<00:23, 15.16it/s]\n",
      "  6%|████▉                                                                            | 23/376 [00:01<00:27, 13.03it/s]\n",
      "  7%|█████▍                                                                           | 25/376 [00:01<00:30, 11.34it/s]\n",
      "  7%|█████▊                                                                           | 27/376 [00:01<00:32, 10.59it/s]\n",
      "  8%|██████▏                                                                          | 29/376 [00:01<00:36,  9.61it/s]\n",
      "  8%|██████▋                                                                          | 31/376 [00:02<00:37,  9.13it/s]\n",
      "  9%|██████▉                                                                          | 32/376 [00:02<00:39,  8.69it/s]\n",
      "  9%|███████                                                                          | 33/376 [00:02<00:42,  7.99it/s]\n",
      "  9%|███████▎                                                                         | 34/376 [00:02<00:43,  7.78it/s]\n",
      "  9%|███████▌                                                                         | 35/376 [00:02<00:45,  7.46it/s]\n",
      " 10%|███████▊                                                                         | 36/376 [00:02<00:48,  6.98it/s]\n",
      " 10%|███████▉                                                                         | 37/376 [00:03<00:51,  6.59it/s]\n",
      " 10%|████████▏                                                                        | 38/376 [00:03<00:54,  6.24it/s]\n",
      " 10%|████████▍                                                                        | 39/376 [00:03<00:55,  6.10it/s]\n",
      " 11%|████████▌                                                                        | 40/376 [00:03<00:59,  5.65it/s]\n",
      " 11%|████████▊                                                                        | 41/376 [00:03<00:59,  5.64it/s]\n",
      " 11%|█████████                                                                        | 42/376 [00:03<00:59,  5.60it/s]\n",
      " 11%|█████████▎                                                                       | 43/376 [00:04<01:02,  5.33it/s]\n",
      " 12%|█████████▍                                                                       | 44/376 [00:04<01:04,  5.18it/s]\n",
      " 12%|█████████▋                                                                       | 45/376 [00:04<01:04,  5.12it/s]\n",
      " 12%|█████████▉                                                                       | 46/376 [00:04<01:05,  5.04it/s]\n",
      " 12%|██████████▏                                                                      | 47/376 [00:04<01:06,  4.94it/s]\n",
      " 13%|██████████▎                                                                      | 48/376 [00:05<01:06,  4.96it/s]\n",
      " 13%|██████████▌                                                                      | 49/376 [00:05<01:07,  4.84it/s]\n",
      " 13%|██████████▊                                                                      | 50/376 [00:05<01:08,  4.76it/s]\n",
      " 14%|██████████▉                                                                      | 51/376 [00:05<01:08,  4.74it/s]\n",
      " 14%|███████████▏                                                                     | 52/376 [00:06<01:09,  4.66it/s]\n",
      " 14%|███████████▍                                                                     | 53/376 [00:06<01:10,  4.59it/s]\n",
      " 14%|███████████▋                                                                     | 54/376 [00:06<01:12,  4.47it/s]\n",
      " 15%|███████████▊                                                                     | 55/376 [00:06<01:12,  4.41it/s]\n",
      " 15%|████████████                                                                     | 56/376 [00:06<01:13,  4.35it/s]\n",
      " 15%|████████████▎                                                                    | 57/376 [00:07<01:14,  4.30it/s]\n",
      " 15%|████████████▍                                                                    | 58/376 [00:07<01:14,  4.27it/s]\n",
      " 16%|████████████▋                                                                    | 59/376 [00:07<01:15,  4.21it/s]\n",
      " 16%|████████████▉                                                                    | 60/376 [00:07<01:16,  4.14it/s]\n",
      " 16%|█████████████▏                                                                   | 61/376 [00:08<01:18,  4.04it/s]\n",
      " 16%|█████████████▎                                                                   | 62/376 [00:08<01:19,  3.97it/s]\n",
      " 17%|█████████████▌                                                                   | 63/376 [00:08<01:19,  3.92it/s]\n",
      " 17%|█████████████▊                                                                   | 64/376 [00:08<01:19,  3.92it/s]\n",
      " 17%|██████████████                                                                   | 65/376 [00:09<01:20,  3.87it/s]\n",
      " 18%|██████████████▏                                                                  | 66/376 [00:09<01:19,  3.90it/s]\n",
      " 18%|██████████████▍                                                                  | 67/376 [00:09<01:19,  3.89it/s]\n",
      " 18%|██████████████▋                                                                  | 68/376 [00:10<01:19,  3.86it/s]\n",
      " 18%|██████████████▊                                                                  | 69/376 [00:10<01:19,  3.85it/s]\n",
      " 19%|███████████████                                                                  | 70/376 [00:10<01:23,  3.68it/s]\n",
      " 19%|███████████████▎                                                                 | 71/376 [00:10<01:23,  3.65it/s]\n",
      " 19%|███████████████▌                                                                 | 72/376 [00:11<01:25,  3.57it/s]\n",
      " 19%|███████████████▋                                                                 | 73/376 [00:11<01:25,  3.52it/s]\n",
      " 20%|███████████████▉                                                                 | 74/376 [00:11<01:26,  3.49it/s]\n",
      " 20%|████████████████▏                                                                | 75/376 [00:12<01:27,  3.44it/s]\n",
      " 20%|████████████████▎                                                                | 76/376 [00:12<01:29,  3.35it/s]\n",
      " 20%|████████████████▌                                                                | 77/376 [00:12<01:30,  3.31it/s]\n",
      " 21%|████████████████▊                                                                | 78/376 [00:12<01:29,  3.35it/s]\n",
      " 21%|█████████████████                                                                | 79/376 [00:13<01:30,  3.30it/s]\n",
      " 21%|█████████████████▏                                                               | 80/376 [00:13<01:30,  3.26it/s]\n",
      " 22%|█████████████████▍                                                               | 81/376 [00:13<01:31,  3.23it/s]\n",
      " 22%|█████████████████▋                                                               | 82/376 [00:14<01:32,  3.19it/s]\n",
      " 22%|█████████████████▉                                                               | 83/376 [00:14<01:32,  3.16it/s]\n",
      " 22%|██████████████████                                                               | 84/376 [00:14<01:35,  3.05it/s]\n",
      " 23%|██████████████████▎                                                              | 85/376 [00:15<01:36,  3.02it/s]\n",
      " 23%|██████████████████▌                                                              | 86/376 [00:15<01:37,  2.98it/s]\n",
      " 23%|██████████████████▋                                                              | 87/376 [00:15<01:38,  2.94it/s]\n",
      " 23%|██████████████████▉                                                              | 88/376 [00:16<01:38,  2.93it/s]\n",
      " 24%|███████████████████▏                                                             | 89/376 [00:16<01:38,  2.91it/s]\n",
      " 24%|███████████████████▍                                                             | 90/376 [00:16<01:39,  2.89it/s]\n",
      " 24%|███████████████████▌                                                             | 91/376 [00:17<01:40,  2.84it/s]\n",
      " 24%|███████████████████▊                                                             | 92/376 [00:17<01:41,  2.80it/s]\n",
      " 25%|████████████████████                                                             | 93/376 [00:18<01:42,  2.76it/s]\n",
      " 25%|████████████████████▎                                                            | 94/376 [00:18<01:43,  2.73it/s]\n",
      " 25%|████████████████████▍                                                            | 95/376 [00:18<01:45,  2.67it/s]\n",
      " 26%|████████████████████▋                                                            | 96/376 [00:19<01:45,  2.65it/s]\n",
      " 26%|████████████████████▉                                                            | 97/376 [00:19<01:47,  2.60it/s]\n",
      " 26%|█████████████████████                                                            | 98/376 [00:20<01:49,  2.55it/s]\n",
      " 26%|█████████████████████▎                                                           | 99/376 [00:20<01:50,  2.51it/s]\n",
      " 27%|█████████████████████▎                                                          | 100/376 [00:20<01:52,  2.46it/s]\n",
      " 27%|█████████████████████▍                                                          | 101/376 [00:21<01:52,  2.44it/s]\n",
      " 27%|█████████████████████▋                                                          | 102/376 [00:21<01:55,  2.38it/s]\n",
      " 27%|█████████████████████▉                                                          | 103/376 [00:22<01:56,  2.35it/s]\n",
      " 28%|██████████████████████▏                                                         | 104/376 [00:22<01:58,  2.29it/s]\n",
      " 28%|██████████████████████▎                                                         | 105/376 [00:23<01:58,  2.29it/s]\n",
      " 28%|██████████████████████▌                                                         | 106/376 [00:23<01:58,  2.27it/s]\n",
      " 28%|██████████████████████▊                                                         | 107/376 [00:24<01:59,  2.25it/s]\n",
      " 29%|██████████████████████▉                                                         | 108/376 [00:24<02:02,  2.19it/s]\n",
      " 29%|███████████████████████▏                                                        | 109/376 [00:24<02:03,  2.17it/s]\n",
      " 29%|███████████████████████▍                                                        | 110/376 [00:25<02:04,  2.13it/s]\n",
      " 30%|███████████████████████▌                                                        | 111/376 [00:25<02:05,  2.12it/s]\n",
      " 30%|███████████████████████▊                                                        | 112/376 [00:26<02:05,  2.11it/s]\n",
      " 30%|████████████████████████                                                        | 113/376 [00:26<02:07,  2.06it/s]\n",
      " 30%|████████████████████████▎                                                       | 114/376 [00:27<02:09,  2.03it/s]\n",
      " 31%|████████████████████████▍                                                       | 115/376 [00:27<02:11,  1.99it/s]\n",
      " 31%|████████████████████████▋                                                       | 116/376 [00:28<02:14,  1.93it/s]\n",
      " 31%|████████████████████████▉                                                       | 117/376 [00:29<02:13,  1.94it/s]\n",
      " 31%|█████████████████████████                                                       | 118/376 [00:29<02:13,  1.93it/s]\n",
      " 32%|█████████████████████████▎                                                      | 119/376 [00:30<02:12,  1.94it/s]\n",
      " 32%|█████████████████████████▌                                                      | 120/376 [00:30<02:15,  1.89it/s]\n",
      " 32%|█████████████████████████▋                                                      | 121/376 [00:31<02:20,  1.82it/s]\n",
      " 32%|█████████████████████████▉                                                      | 122/376 [00:31<02:24,  1.75it/s]\n",
      " 33%|██████████████████████████▏                                                     | 123/376 [00:32<02:27,  1.71it/s]\n",
      " 33%|██████████████████████████▍                                                     | 124/376 [00:33<02:35,  1.62it/s]\n",
      " 33%|██████████████████████████▌                                                     | 125/376 [00:33<02:35,  1.62it/s]\n",
      " 34%|██████████████████████████▊                                                     | 126/376 [00:34<02:35,  1.61it/s]\n",
      " 34%|███████████████████████████                                                     | 127/376 [00:35<02:37,  1.58it/s]\n",
      " 34%|███████████████████████████▏                                                    | 128/376 [00:35<02:42,  1.53it/s]\n",
      " 34%|███████████████████████████▍                                                    | 129/376 [00:36<02:45,  1.49it/s]\n",
      " 35%|███████████████████████████▋                                                    | 130/376 [00:37<02:48,  1.46it/s]\n",
      " 35%|███████████████████████████▊                                                    | 131/376 [00:37<02:47,  1.46it/s]\n",
      " 35%|████████████████████████████                                                    | 132/376 [00:38<02:44,  1.48it/s]\n",
      " 35%|████████████████████████████▎                                                   | 133/376 [00:39<02:42,  1.49it/s]\n",
      " 36%|████████████████████████████▌                                                   | 134/376 [00:39<02:45,  1.47it/s]\n",
      " 36%|████████████████████████████▋                                                   | 135/376 [00:40<02:45,  1.45it/s]\n",
      " 36%|████████████████████████████▉                                                   | 136/376 [00:41<02:46,  1.44it/s]\n",
      " 36%|█████████████████████████████▏                                                  | 137/376 [00:41<02:45,  1.45it/s]\n",
      " 37%|█████████████████████████████▎                                                  | 138/376 [00:42<02:43,  1.46it/s]\n",
      " 37%|█████████████████████████████▌                                                  | 139/376 [00:43<02:41,  1.47it/s]\n",
      " 37%|█████████████████████████████▊                                                  | 140/376 [00:44<02:41,  1.46it/s]\n",
      " 38%|██████████████████████████████                                                  | 141/376 [00:44<02:40,  1.46it/s]\n",
      " 38%|██████████████████████████████▏                                                 | 142/376 [00:45<02:38,  1.47it/s]\n",
      " 38%|██████████████████████████████▍                                                 | 143/376 [00:46<02:38,  1.47it/s]\n",
      " 38%|██████████████████████████████▋                                                 | 144/376 [00:46<02:38,  1.46it/s]\n",
      " 39%|██████████████████████████████▊                                                 | 145/376 [00:47<02:41,  1.43it/s]\n",
      " 39%|███████████████████████████████                                                 | 146/376 [00:48<03:16,  1.17it/s]\n",
      " 39%|███████████████████████████████▎                                                | 147/376 [00:49<03:22,  1.13it/s]\n",
      " 39%|███████████████████████████████▍                                                | 148/376 [00:50<03:11,  1.19it/s]\n",
      " 40%|███████████████████████████████▋                                                | 149/376 [00:51<03:04,  1.23it/s]\n",
      " 40%|███████████████████████████████▉                                                | 150/376 [00:51<02:58,  1.27it/s]\n",
      " 40%|████████████████████████████████▏                                               | 151/376 [00:52<02:56,  1.28it/s]\n",
      " 40%|████████████████████████████████▎                                               | 152/376 [00:53<02:53,  1.29it/s]\n",
      " 41%|████████████████████████████████▌                                               | 153/376 [00:54<02:55,  1.27it/s]\n",
      " 41%|████████████████████████████████▊                                               | 154/376 [00:54<02:52,  1.29it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████████████████████▉                                               | 155/376 [00:55<02:54,  1.26it/s]\n",
      " 41%|█████████████████████████████████▏                                              | 156/376 [00:56<02:55,  1.25it/s]\n",
      " 42%|█████████████████████████████████▍                                              | 157/376 [00:57<02:52,  1.27it/s]\n",
      " 42%|█████████████████████████████████▌                                              | 158/376 [00:58<02:50,  1.28it/s]\n",
      " 42%|█████████████████████████████████▊                                              | 159/376 [00:58<02:50,  1.27it/s]\n",
      " 43%|██████████████████████████████████                                              | 160/376 [00:59<02:49,  1.27it/s]\n",
      " 43%|██████████████████████████████████▎                                             | 161/376 [01:00<02:48,  1.28it/s]\n",
      " 43%|██████████████████████████████████▍                                             | 162/376 [01:01<02:48,  1.27it/s]\n",
      " 43%|██████████████████████████████████▋                                             | 163/376 [01:02<02:47,  1.27it/s]\n",
      " 44%|██████████████████████████████████▉                                             | 164/376 [01:02<02:47,  1.26it/s]\n",
      " 44%|███████████████████████████████████                                             | 165/376 [01:03<02:47,  1.26it/s]\n",
      " 44%|███████████████████████████████████▎                                            | 166/376 [01:04<02:48,  1.25it/s]\n",
      " 44%|███████████████████████████████████▌                                            | 167/376 [01:05<02:47,  1.24it/s]\n",
      " 45%|███████████████████████████████████▋                                            | 168/376 [01:06<02:48,  1.23it/s]\n",
      " 45%|███████████████████████████████████▉                                            | 169/376 [01:06<02:51,  1.21it/s]\n",
      " 45%|████████████████████████████████████▏                                           | 170/376 [01:07<02:50,  1.21it/s]\n",
      " 45%|████████████████████████████████████▍                                           | 171/376 [01:08<02:50,  1.21it/s]\n",
      " 46%|████████████████████████████████████▌                                           | 172/376 [01:09<02:52,  1.19it/s]\n",
      " 46%|████████████████████████████████████▊                                           | 173/376 [01:10<02:53,  1.17it/s]\n",
      " 46%|█████████████████████████████████████                                           | 174/376 [01:11<02:52,  1.17it/s]\n",
      " 47%|█████████████████████████████████████▏                                          | 175/376 [01:12<02:51,  1.17it/s]\n",
      " 47%|█████████████████████████████████████▍                                          | 176/376 [01:13<02:53,  1.15it/s]\n",
      " 47%|█████████████████████████████████████▋                                          | 177/376 [01:13<02:54,  1.14it/s]\n",
      " 47%|█████████████████████████████████████▊                                          | 178/376 [01:14<02:57,  1.12it/s]\n",
      " 48%|██████████████████████████████████████                                          | 179/376 [01:15<02:57,  1.11it/s]\n",
      " 48%|██████████████████████████████████████▎                                         | 180/376 [01:16<02:55,  1.12it/s]\n",
      " 48%|██████████████████████████████████████▌                                         | 181/376 [01:17<02:54,  1.11it/s]\n",
      " 48%|██████████████████████████████████████▋                                         | 182/376 [01:18<02:54,  1.11it/s]\n",
      " 49%|██████████████████████████████████████▉                                         | 183/376 [01:19<02:53,  1.11it/s]\n",
      " 49%|███████████████████████████████████████▏                                        | 184/376 [01:20<02:51,  1.12it/s]\n",
      " 49%|███████████████████████████████████████▎                                        | 185/376 [01:21<02:53,  1.10it/s]\n",
      " 49%|███████████████████████████████████████▌                                        | 186/376 [01:22<02:59,  1.06it/s]\n",
      " 50%|███████████████████████████████████████▊                                        | 187/376 [01:23<03:22,  1.07s/it]\n",
      " 50%|████████████████████████████████████████                                        | 188/376 [01:24<03:16,  1.04s/it]\n",
      " 50%|████████████████████████████████████████▏                                       | 189/376 [01:25<03:11,  1.02s/it]\n",
      " 51%|████████████████████████████████████████▍                                       | 190/376 [01:26<03:24,  1.10s/it]\n",
      " 51%|████████████████████████████████████████▋                                       | 191/376 [01:27<03:16,  1.06s/it]\n",
      " 51%|████████████████████████████████████████▊                                       | 192/376 [01:28<03:09,  1.03s/it]\n",
      " 51%|█████████████████████████████████████████                                       | 193/376 [01:29<03:03,  1.00s/it]\n",
      " 52%|█████████████████████████████████████████▎                                      | 194/376 [01:30<03:00,  1.01it/s]\n",
      " 52%|█████████████████████████████████████████▍                                      | 195/376 [01:31<03:02,  1.01s/it]\n",
      " 52%|█████████████████████████████████████████▋                                      | 196/376 [01:32<03:02,  1.01s/it]\n",
      " 52%|█████████████████████████████████████████▉                                      | 197/376 [01:33<02:58,  1.00it/s]\n",
      " 53%|██████████████████████████████████████████▏                                     | 198/376 [01:34<02:56,  1.01it/s]\n",
      " 53%|██████████████████████████████████████████▎                                     | 199/376 [01:35<02:56,  1.00it/s]\n",
      " 53%|██████████████████████████████████████████▌                                     | 200/376 [01:36<02:55,  1.00it/s]\n",
      " 53%|██████████████████████████████████████████▊                                     | 201/376 [01:37<03:00,  1.03s/it]\n",
      " 54%|██████████████████████████████████████████▉                                     | 202/376 [01:38<02:59,  1.03s/it]\n",
      " 54%|███████████████████████████████████████████▏                                    | 203/376 [01:39<02:55,  1.02s/it]\n",
      " 54%|███████████████████████████████████████████▍                                    | 204/376 [01:40<02:55,  1.02s/it]\n",
      " 55%|███████████████████████████████████████████▌                                    | 205/376 [01:41<02:54,  1.02s/it]\n",
      " 55%|███████████████████████████████████████████▊                                    | 206/376 [01:43<03:02,  1.08s/it]\n",
      " 55%|████████████████████████████████████████████                                    | 207/376 [01:44<03:06,  1.11s/it]\n",
      " 55%|████████████████████████████████████████████▎                                   | 208/376 [01:45<03:00,  1.08s/it]\n",
      " 56%|████████████████████████████████████████████▍                                   | 209/376 [01:46<02:56,  1.05s/it]\n",
      " 56%|████████████████████████████████████████████▋                                   | 210/376 [01:47<02:52,  1.04s/it]\n",
      " 56%|████████████████████████████████████████████▉                                   | 211/376 [01:48<02:49,  1.03s/it]\n",
      " 56%|█████████████████████████████████████████████                                   | 212/376 [01:49<02:49,  1.03s/it]\n",
      " 57%|█████████████████████████████████████████████▎                                  | 213/376 [01:50<02:53,  1.07s/it]\n",
      " 57%|█████████████████████████████████████████████▌                                  | 214/376 [01:51<02:51,  1.06s/it]\n",
      " 57%|█████████████████████████████████████████████▋                                  | 215/376 [01:52<02:49,  1.05s/it]\n",
      " 57%|█████████████████████████████████████████████▉                                  | 216/376 [01:53<02:48,  1.05s/it]\n",
      " 58%|██████████████████████████████████████████████▏                                 | 217/376 [01:54<02:48,  1.06s/it]\n",
      " 58%|██████████████████████████████████████████████▍                                 | 218/376 [01:55<02:49,  1.07s/it]"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-b3f304cd1d2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'POS'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag_sents\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_stop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m                 \u001b[1;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m                 \u001b[1;31m# Close bar and return pandas calculation result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4042\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m                     \u001b[1;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' \\1 \\2 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' \\1 \\2 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in df_list:\n",
    "    i['POS'] = pos_tag_sents( i['text_stop'].progress_apply(word_tokenize).tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['token_pos'] = train['POS'].progress_apply(lambda row:' '.join([i[0] for i in row if i[1] in ('JJ','JJR','JJS','RB','RBR','RBS','NNP','NN','NNS','NNPS','VB','VBD','VBG','VBN','VBZ','VBP')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['token_nouns'] = train['POS'].apply(lambda row:' '.join([i[0] for i in row if i[1] in ('JJ','JJR','JJS','NNP','NN','NNS','NNPS')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens=[]\n",
    "# #pos=[]\n",
    "# #lemma=[]\n",
    "# token_pos=[]\n",
    "# token_noun=[]\n",
    "\n",
    "\n",
    "\n",
    "# for doc in nlp.pipe(df['job_description'].astype('unicode').values,batch_size=500,n_threads=3):\n",
    "#     if doc.is_parsed:   #is_parsed, bool, A flag indicating that the document has been syntactically parsed.\n",
    "#         tokens.append([n.text for n in doc])\n",
    "#         #pos.append([n.pos_ for n in doc])\n",
    "#         token_pos.append(' '.join([n.text for n in doc if n.pos_ in ('NOUN','ADJ','VERB','ADV')]))\n",
    "#         token_noun.append(' '.join([n.text for n in doc if n.pos_ in ('NOUN')]))\n",
    "\n",
    "#         #lemma.append([n.lemma_ for n in doc])\n",
    "        \n",
    "#     else:\n",
    "#         # We want to make sure that the lists of parsed results have the\n",
    "#         # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "#         tokens.append(None)\n",
    "#         #pos.append(None)\n",
    "#         token_pos.append(None)\n",
    "#         token_noun.append(None)\n",
    "\n",
    "        \n",
    "\n",
    "# df=df.assign(tokens=pd.Series(tokens))\n",
    "# #df=df.assign(pos=pd.Series(pos))\n",
    "# #df=df.assign(lemma=pd.Series(lemma))\n",
    "# df=df.assign(token_pos=pd.Series(token_pos))\n",
    "# df=df.assign(token_noun=pd.Series(token_noun))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.token_pos.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_title', 'job_description', 'clean_text', 'text_stop',\n",
       "       'tokenized_text', 'POS', 'Token_Nouns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=df['Token_Nouns'].dropna().tolist()#list of positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(stop_words=my_stop_words,ngram_range=(1, 3))\n",
    "tfidf_vec = tf_idf.fit_transform(corpus)\n",
    "#idf_df = pd.DataFrame(tfidf_vec.toarray(), columns=tf_idf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(8, algorithm=\"arpack\" )\n",
    "lsa_vecs = lsa.fit_transform(tfidf_vec)\n",
    "lsa_vecs = Normalizer(copy=False).fit_transform(lsa_vecs)\n",
    "feature_names = tf_idf.get_feature_names()\n",
    "#lsa_df = pd.DataFrame(lsa.components_.round(5), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0 words</th>\n",
       "      <th>Topic 0 weights</th>\n",
       "      <th>Topic 1 words</th>\n",
       "      <th>Topic 1 weights</th>\n",
       "      <th>Topic 2 words</th>\n",
       "      <th>Topic 2 weights</th>\n",
       "      <th>Topic 3 words</th>\n",
       "      <th>Topic 3 weights</th>\n",
       "      <th>Topic 4 words</th>\n",
       "      <th>Topic 4 weights</th>\n",
       "      <th>Topic 5 words</th>\n",
       "      <th>Topic 5 weights</th>\n",
       "      <th>Topic 6 words</th>\n",
       "      <th>Topic 6 weights</th>\n",
       "      <th>Topic 7 words</th>\n",
       "      <th>Topic 7 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data</td>\n",
       "      <td>0.3</td>\n",
       "      <td>requirements</td>\n",
       "      <td>0.1</td>\n",
       "      <td>requirements</td>\n",
       "      <td>0.1</td>\n",
       "      <td>data</td>\n",
       "      <td>0.2</td>\n",
       "      <td>seo</td>\n",
       "      <td>0.2</td>\n",
       "      <td>business</td>\n",
       "      <td>0.1</td>\n",
       "      <td>big data</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sales</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>using</td>\n",
       "      <td>0.2</td>\n",
       "      <td>work</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ensure requirements</td>\n",
       "      <td>0.1</td>\n",
       "      <td>crm</td>\n",
       "      <td>0.1</td>\n",
       "      <td>marketing</td>\n",
       "      <td>0.2</td>\n",
       "      <td>analytics</td>\n",
       "      <td>0.1</td>\n",
       "      <td>big</td>\n",
       "      <td>0.1</td>\n",
       "      <td>data analysis statistics</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>learning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>business</td>\n",
       "      <td>0.1</td>\n",
       "      <td>capturing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>founder</td>\n",
       "      <td>0.1</td>\n",
       "      <td>search</td>\n",
       "      <td>0.2</td>\n",
       "      <td>data</td>\n",
       "      <td>0.1</td>\n",
       "      <td>big data machine</td>\n",
       "      <td>0.1</td>\n",
       "      <td>analysis statistics finance</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>marketing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>data</td>\n",
       "      <td>0.1</td>\n",
       "      <td>met</td>\n",
       "      <td>0.1</td>\n",
       "      <td>data architect</td>\n",
       "      <td>0.1</td>\n",
       "      <td>time job</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sales</td>\n",
       "      <td>0.1</td>\n",
       "      <td>data management advanced</td>\n",
       "      <td>0.1</td>\n",
       "      <td>statistics finance business</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>models</td>\n",
       "      <td>0.1</td>\n",
       "      <td>analytics</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ensure</td>\n",
       "      <td>0.1</td>\n",
       "      <td>years required</td>\n",
       "      <td>0.1</td>\n",
       "      <td>search engine</td>\n",
       "      <td>0.1</td>\n",
       "      <td>technology</td>\n",
       "      <td>0.1</td>\n",
       "      <td>transitioned clients</td>\n",
       "      <td>0.1</td>\n",
       "      <td>statistics finance</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>experience</td>\n",
       "      <td>0.1</td>\n",
       "      <td>development</td>\n",
       "      <td>0.1</td>\n",
       "      <td>gathering</td>\n",
       "      <td>0.1</td>\n",
       "      <td>architect</td>\n",
       "      <td>0.1</td>\n",
       "      <td>social media marketing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>management</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mining bi</td>\n",
       "      <td>0.1</td>\n",
       "      <td>analysis statistics</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>sales</td>\n",
       "      <td>0.1</td>\n",
       "      <td>innovation</td>\n",
       "      <td>0.1</td>\n",
       "      <td>development</td>\n",
       "      <td>0.1</td>\n",
       "      <td>vcs</td>\n",
       "      <td>0.1</td>\n",
       "      <td>engine optimization</td>\n",
       "      <td>0.1</td>\n",
       "      <td>team</td>\n",
       "      <td>0.1</td>\n",
       "      <td>experiencetredence analytics</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sales data analysis</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>business</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ensure requirements</td>\n",
       "      <td>0.1</td>\n",
       "      <td>software development</td>\n",
       "      <td>0.1</td>\n",
       "      <td>data architecture</td>\n",
       "      <td>0.1</td>\n",
       "      <td>search engine optimization</td>\n",
       "      <td>0.1</td>\n",
       "      <td>analysis</td>\n",
       "      <td>0.1</td>\n",
       "      <td>experiencetredence</td>\n",
       "      <td>0.1</td>\n",
       "      <td>shift</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>machine learning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>care</td>\n",
       "      <td>0.1</td>\n",
       "      <td>business requirements</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mysql</td>\n",
       "      <td>0.1</td>\n",
       "      <td>media marketing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>experience</td>\n",
       "      <td>0.1</td>\n",
       "      <td>learning bringing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>finance business</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>machine</td>\n",
       "      <td>0.1</td>\n",
       "      <td>capturing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>document</td>\n",
       "      <td>0.0</td>\n",
       "      <td>backed</td>\n",
       "      <td>0.1</td>\n",
       "      <td>time</td>\n",
       "      <td>0.1</td>\n",
       "      <td>solutions</td>\n",
       "      <td>0.1</td>\n",
       "      <td>existing technology data</td>\n",
       "      <td>0.1</td>\n",
       "      <td>status</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>experience using</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ensure</td>\n",
       "      <td>0.1</td>\n",
       "      <td>practices</td>\n",
       "      <td>0.0</td>\n",
       "      <td>structure</td>\n",
       "      <td>0.1</td>\n",
       "      <td>social media</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bengaluru</td>\n",
       "      <td>0.1</td>\n",
       "      <td>capabilities range</td>\n",
       "      <td>0.1</td>\n",
       "      <td>data analysis</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.1</td>\n",
       "      <td>met</td>\n",
       "      <td>0.1</td>\n",
       "      <td>solution</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mis</td>\n",
       "      <td>0.1</td>\n",
       "      <td>digital marketing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>services</td>\n",
       "      <td>0.1</td>\n",
       "      <td>capabilities range data</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sales data</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>ds</td>\n",
       "      <td>0.1</td>\n",
       "      <td>insight</td>\n",
       "      <td>0.1</td>\n",
       "      <td>client</td>\n",
       "      <td>0.0</td>\n",
       "      <td>required</td>\n",
       "      <td>0.1</td>\n",
       "      <td>paid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>skills</td>\n",
       "      <td>0.1</td>\n",
       "      <td>execution model</td>\n",
       "      <td>0.1</td>\n",
       "      <td>gender</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>tools</td>\n",
       "      <td>0.1</td>\n",
       "      <td>trade</td>\n",
       "      <td>0.1</td>\n",
       "      <td>work experience</td>\n",
       "      <td>0.0</td>\n",
       "      <td>approach</td>\n",
       "      <td>0.1</td>\n",
       "      <td>promotion</td>\n",
       "      <td>0.1</td>\n",
       "      <td>clients</td>\n",
       "      <td>0.1</td>\n",
       "      <td>execution model leveraging</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sales performance</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>sales marketing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>gathering</td>\n",
       "      <td>0.1</td>\n",
       "      <td>stakeholders</td>\n",
       "      <td>0.0</td>\n",
       "      <td>architecture</td>\n",
       "      <td>0.1</td>\n",
       "      <td>social</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bengaluru years</td>\n",
       "      <td>0.1</td>\n",
       "      <td>transitioned clients end</td>\n",
       "      <td>0.1</td>\n",
       "      <td>education</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>drive</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lead</td>\n",
       "      <td>0.1</td>\n",
       "      <td>software</td>\n",
       "      <td>0.0</td>\n",
       "      <td>graduate</td>\n",
       "      <td>0.1</td>\n",
       "      <td>engine</td>\n",
       "      <td>0.1</td>\n",
       "      <td>years</td>\n",
       "      <td>0.1</td>\n",
       "      <td>leveraging clients existing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>statement mcafee</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>analytics</td>\n",
       "      <td>0.1</td>\n",
       "      <td>software development</td>\n",
       "      <td>0.1</td>\n",
       "      <td>use</td>\n",
       "      <td>0.0</td>\n",
       "      <td>objectives</td>\n",
       "      <td>0.1</td>\n",
       "      <td>jan</td>\n",
       "      <td>0.1</td>\n",
       "      <td>big</td>\n",
       "      <td>0.1</td>\n",
       "      <td>create box solutions</td>\n",
       "      <td>0.1</td>\n",
       "      <td>analysis reportingexperience using</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>insights</td>\n",
       "      <td>0.1</td>\n",
       "      <td>consumer</td>\n",
       "      <td>0.1</td>\n",
       "      <td>experience</td>\n",
       "      <td>0.0</td>\n",
       "      <td>set</td>\n",
       "      <td>0.1</td>\n",
       "      <td>learn</td>\n",
       "      <td>0.1</td>\n",
       "      <td>big data</td>\n",
       "      <td>0.1</td>\n",
       "      <td>technology data assets</td>\n",
       "      <td>0.1</td>\n",
       "      <td>insights sales</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>analysis</td>\n",
       "      <td>0.1</td>\n",
       "      <td>business requirements</td>\n",
       "      <td>0.1</td>\n",
       "      <td>problem</td>\n",
       "      <td>0.0</td>\n",
       "      <td>business</td>\n",
       "      <td>0.0</td>\n",
       "      <td>content</td>\n",
       "      <td>0.1</td>\n",
       "      <td>execution</td>\n",
       "      <td>0.1</td>\n",
       "      <td>clients end</td>\n",
       "      <td>0.1</td>\n",
       "      <td>insights sales datasolid</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>stakeholders</td>\n",
       "      <td>0.1</td>\n",
       "      <td>data analytics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>problem solving</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reports</td>\n",
       "      <td>0.0</td>\n",
       "      <td>brand</td>\n",
       "      <td>0.1</td>\n",
       "      <td>technology business analytics</td>\n",
       "      <td>0.1</td>\n",
       "      <td>effectively using execution</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mcafee prohibits discrimination</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>data sets</td>\n",
       "      <td>0.1</td>\n",
       "      <td>solution</td>\n",
       "      <td>0.0</td>\n",
       "      <td>industry</td>\n",
       "      <td>0.0</td>\n",
       "      <td>end</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hyderabad</td>\n",
       "      <td>0.1</td>\n",
       "      <td>capabilities</td>\n",
       "      <td>0.1</td>\n",
       "      <td>create box</td>\n",
       "      <td>0.1</td>\n",
       "      <td>analysis reportingexperience</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>techniques</td>\n",
       "      <td>0.1</td>\n",
       "      <td>practices</td>\n",
       "      <td>0.0</td>\n",
       "      <td>solving</td>\n",
       "      <td>0.0</td>\n",
       "      <td>years</td>\n",
       "      <td>0.0</td>\n",
       "      <td>digital</td>\n",
       "      <td>0.1</td>\n",
       "      <td>technology business</td>\n",
       "      <td>0.0</td>\n",
       "      <td>clients end engagement</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mcafee prohibits</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>ai</td>\n",
       "      <td>0.1</td>\n",
       "      <td>media trade</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pre agreed formats</td>\n",
       "      <td>0.0</td>\n",
       "      <td>seo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>designing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>clients existing</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mcafee</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>work</td>\n",
       "      <td>0.1</td>\n",
       "      <td>receive training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>use case model</td>\n",
       "      <td>0.0</td>\n",
       "      <td>insights</td>\n",
       "      <td>0.0</td>\n",
       "      <td>media</td>\n",
       "      <td>0.1</td>\n",
       "      <td>status</td>\n",
       "      <td>0.0</td>\n",
       "      <td>effectively using</td>\n",
       "      <td>0.1</td>\n",
       "      <td>prohibits discrimination based</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>sets</td>\n",
       "      <td>0.1</td>\n",
       "      <td>analytics consumer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>relationships engage</td>\n",
       "      <td>0.0</td>\n",
       "      <td>analytics structure</td>\n",
       "      <td>0.0</td>\n",
       "      <td>job</td>\n",
       "      <td>0.1</td>\n",
       "      <td>assets</td>\n",
       "      <td>0.0</td>\n",
       "      <td>clients existing technology</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sfdc knowledge</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic 0 words Topic 0 weights          Topic 1 words Topic 1 weights  \\\n",
       "0   data              0.3             requirements           0.1              \n",
       "1   using             0.2             work                   0.1              \n",
       "2   learning          0.1             business               0.1              \n",
       "3   marketing         0.1             data                   0.1              \n",
       "4   models            0.1             analytics              0.1              \n",
       "5   experience        0.1             development            0.1              \n",
       "6   sales             0.1             innovation             0.1              \n",
       "7   business          0.1             ensure requirements    0.1              \n",
       "8   machine learning  0.1             care                   0.1              \n",
       "9   machine           0.1             capturing              0.1              \n",
       "10  experience using  0.1             ensure                 0.1              \n",
       "11  regression        0.1             met                    0.1              \n",
       "12  ds                0.1             insight                0.1              \n",
       "13  tools             0.1             trade                  0.1              \n",
       "14  sales marketing   0.1             gathering              0.1              \n",
       "15  drive             0.1             lead                   0.1              \n",
       "16  analytics         0.1             software development   0.1              \n",
       "17  insights          0.1             consumer               0.1              \n",
       "18  analysis          0.1             business requirements  0.1              \n",
       "19  stakeholders      0.1             data analytics         0.0              \n",
       "20  data sets         0.1             solution               0.0              \n",
       "21  techniques        0.1             practices              0.0              \n",
       "22  ai                0.1             media trade            0.0              \n",
       "23  work              0.1             receive training       0.0              \n",
       "24  sets              0.1             analytics consumer     0.0              \n",
       "\n",
       "            Topic 2 words Topic 2 weights        Topic 3 words  \\\n",
       "0   requirements           0.1             data                  \n",
       "1   ensure requirements    0.1             crm                   \n",
       "2   capturing              0.1             founder               \n",
       "3   met                    0.1             data architect        \n",
       "4   ensure                 0.1             years required        \n",
       "5   gathering              0.1             architect             \n",
       "6   development            0.1             vcs                   \n",
       "7   software development   0.1             data architecture     \n",
       "8   business requirements  0.1             mysql                 \n",
       "9   document               0.0             backed                \n",
       "10  practices              0.0             structure             \n",
       "11  solution               0.0             mis                   \n",
       "12  client                 0.0             required              \n",
       "13  work experience        0.0             approach              \n",
       "14  stakeholders           0.0             architecture          \n",
       "15  software               0.0             graduate              \n",
       "16  use                    0.0             objectives            \n",
       "17  experience             0.0             set                   \n",
       "18  problem                0.0             business              \n",
       "19  problem solving        0.0             reports               \n",
       "20  industry               0.0             end                   \n",
       "21  solving                0.0             years                 \n",
       "22  pre agreed formats     0.0             seo                   \n",
       "23  use case model         0.0             insights              \n",
       "24  relationships engage   0.0             analytics structure   \n",
       "\n",
       "   Topic 3 weights               Topic 4 words Topic 4 weights  \\\n",
       "0   0.2             seo                         0.2              \n",
       "1   0.1             marketing                   0.2              \n",
       "2   0.1             search                      0.2              \n",
       "3   0.1             time job                    0.1              \n",
       "4   0.1             search engine               0.1              \n",
       "5   0.1             social media marketing      0.1              \n",
       "6   0.1             engine optimization         0.1              \n",
       "7   0.1             search engine optimization  0.1              \n",
       "8   0.1             media marketing             0.1              \n",
       "9   0.1             time                        0.1              \n",
       "10  0.1             social media                0.1              \n",
       "11  0.1             digital marketing           0.1              \n",
       "12  0.1             paid                        0.1              \n",
       "13  0.1             promotion                   0.1              \n",
       "14  0.1             social                      0.1              \n",
       "15  0.1             engine                      0.1              \n",
       "16  0.1             jan                         0.1              \n",
       "17  0.1             learn                       0.1              \n",
       "18  0.0             content                     0.1              \n",
       "19  0.0             brand                       0.1              \n",
       "20  0.0             hyderabad                   0.1              \n",
       "21  0.0             digital                     0.1              \n",
       "22  0.0             designing                   0.1              \n",
       "23  0.0             media                       0.1              \n",
       "24  0.0             job                         0.1              \n",
       "\n",
       "                    Topic 5 words Topic 5 weights  \\\n",
       "0   business                       0.1              \n",
       "1   analytics                      0.1              \n",
       "2   data                           0.1              \n",
       "3   sales                          0.1              \n",
       "4   technology                     0.1              \n",
       "5   management                     0.1              \n",
       "6   team                           0.1              \n",
       "7   analysis                       0.1              \n",
       "8   experience                     0.1              \n",
       "9   solutions                      0.1              \n",
       "10  bengaluru                      0.1              \n",
       "11  services                       0.1              \n",
       "12  skills                         0.1              \n",
       "13  clients                        0.1              \n",
       "14  bengaluru years                0.1              \n",
       "15  years                          0.1              \n",
       "16  big                            0.1              \n",
       "17  big data                       0.1              \n",
       "18  execution                      0.1              \n",
       "19  technology business analytics  0.1              \n",
       "20  capabilities                   0.1              \n",
       "21  technology business            0.0              \n",
       "22  bi                             0.0              \n",
       "23  status                         0.0              \n",
       "24  assets                         0.0              \n",
       "\n",
       "                   Topic 6 words Topic 6 weights  \\\n",
       "0   big data                      0.1              \n",
       "1   big                           0.1              \n",
       "2   big data machine              0.1              \n",
       "3   data management advanced      0.1              \n",
       "4   transitioned clients          0.1              \n",
       "5   mining bi                     0.1              \n",
       "6   experiencetredence analytics  0.1              \n",
       "7   experiencetredence            0.1              \n",
       "8   learning bringing             0.1              \n",
       "9   existing technology data      0.1              \n",
       "10  capabilities range            0.1              \n",
       "11  capabilities range data       0.1              \n",
       "12  execution model               0.1              \n",
       "13  execution model leveraging    0.1              \n",
       "14  transitioned clients end      0.1              \n",
       "15  leveraging clients existing   0.1              \n",
       "16  create box solutions          0.1              \n",
       "17  technology data assets        0.1              \n",
       "18  clients end                   0.1              \n",
       "19  effectively using execution   0.1              \n",
       "20  create box                    0.1              \n",
       "21  clients end engagement        0.1              \n",
       "22  clients existing              0.1              \n",
       "23  effectively using             0.1              \n",
       "24  clients existing technology   0.1              \n",
       "\n",
       "                         Topic 7 words Topic 7 weights  \n",
       "0   sales                               0.2             \n",
       "1   data analysis statistics            0.1             \n",
       "2   analysis statistics finance         0.1             \n",
       "3   statistics finance business         0.1             \n",
       "4   statistics finance                  0.1             \n",
       "5   analysis statistics                 0.1             \n",
       "6   sales data analysis                 0.1             \n",
       "7   shift                               0.1             \n",
       "8   finance business                    0.1             \n",
       "9   status                              0.1             \n",
       "10  data analysis                       0.1             \n",
       "11  sales data                          0.1             \n",
       "12  gender                              0.1             \n",
       "13  sales performance                   0.1             \n",
       "14  education                           0.0             \n",
       "15  statement mcafee                    0.0             \n",
       "16  analysis reportingexperience using  0.0             \n",
       "17  insights sales                      0.0             \n",
       "18  insights sales datasolid            0.0             \n",
       "19  mcafee prohibits discrimination     0.0             \n",
       "20  analysis reportingexperience        0.0             \n",
       "21  mcafee prohibits                    0.0             \n",
       "22  mcafee                              0.0             \n",
       "23  prohibits discrimination based      0.0             \n",
       "24  sfdc knowledge                      0.0             "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "no_top_words = 25\n",
    "display_topics(lsa, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"concentration\",\"apply\",\"proven\",\"bangalore\",\"mumbai\",\"karnataka\",\"maharashtra\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document clustering with Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10 #Change it according to your data.\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_vec)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "\n",
    "\n",
    "idea={'Idea':corpus, 'Cluster':clusters} #Creating dict having doc with the corresponding cluster number.\n",
    "frame=pd.DataFrame(idea,index=[clusters], columns=['Idea','Cluster']) # Converting it into a dataframe.\n",
    "\n",
    "\n",
    "#frame['Cluster'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    756\n",
       "1    114\n",
       "7    75 \n",
       "8    60 \n",
       "2    60 \n",
       "9    58 \n",
       "6    58 \n",
       "3    45 \n",
       "5    43 \n",
       "4    28 \n",
       "Name: Cluster, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['Cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster: \n",
      "\n",
      "Cluster 0: sales marketing design project performance problems products reporting software processes engineering people environment operations research reports supply chain planning supply chain client clients risk ensure level status quality building dashboards strategy service position market finance systems india manage testing excel business intelligence impact banking platform degree analyst manager bi organization ms trust inventory technologies end leading enterprise making model amazon bengaluru program strong techniques define etl northern trust northern deliver tableau com decision understand delivery methods candidate highly communication skills needs related members applications target expertise cloud metrics practices lead application opportunity areas effectively driving managing results skills ability day focus goals hands great description\n",
      "\n",
      "Cluster 1: marketing sales experience using regression machine learning machine ds sales marketing data sets ai sets techniques platforms optimization using variety data using variety company data boosting variety data forest random forest random data analysis drive business ml data sources variety leveraging methods sources using data languages hadoop work years preferred tableau years preferred candidate data science work years results creating communicate data mining type master looker build analytics solutions research company data communicate stakeholders communicate results looker product sales leadership looker build evaluate company data drive stakeholders communicate results ideas science ds artificial looking support product looking support results data based science ds cross validation method presentation stakeholders communicate learn master technologies learn master cross validation advanced data science presentation stakeholders opportunities leveraging company pipelines systems work unsupervised elastic net regression global marketing sales global marketing analyzing company analyzing company data unsupervised learning world unsupervised learning information using data marketing organization information using frameworks sas frameworks sas tera ph computer science ph computer marketing teams insights creating running simulations elastic net marketing optimization sales pipelines systems tests experience applications stakeholders identify opportunities marketing analytics written stakeholders improve stakeholders improve business bagging random forest bagging random bagging sets working marketing optimization stakeholders teams candidate\n",
      "\n",
      "Cluster 2: fidelity research fmr investment investments bangalore teaching cmdb employee fidelity investments semesters years semesters aggregate semesters years aggregate semesters aggregate minimum marks marks headquartered managed learning quality execution bengaluru karnatakato ensure quality teaching learning quality bengaluru karnatakato ensure closely faculties university bangalore refer work closely faculties quality execution quality execution teachingcentre excellence work closely teaching learning teaching learning quality jain university bangalore profile business analytics execution teachingcentre business analytics location analytics location jain profile business faculties faculties research faculties research profile closely faculties research teachingcentre excellence teachingcentre quality teaching learning location jain jain university jayanagar bengaluru karnatakato karnatakato karnatakato ensure jayanagar bengaluru karnatakato ensure quality research profile location jain university bangalore refer job execution teachingcentre excellence research profile business quality teaching jayanagar teachingcentre excellence work university bangalore bangalore refer investment product analytics location refer job description refer job jain excellence work quality fmr india refer ensure quality india work closely grade aggregate marks grade minimum marks grade marks grade aggregate grade aggregate semesters age limit associates bengaluru limit university iit age ii pm people analytics post profile investing ensure focused minimum description expertise closely people techniques\n",
      "\n",
      "Cluster 3: crm founder data architect years required architect vcs data architecture mysql backed mumbai maharashtra maharashtra structure mis mumbai approach architecture graduate objectives set data analysis reports end mission data starqualifications mission data business drivers backed creating communication presentation mis specialist join creating communication objectives data architecture results targeting billion develop achieve develop achieve term database solutions store passionate data architect informationpreparing reports executive objectives data incredibly opportunity passionate responsibilities dutieslead incredibly opportunity seggregation seggregation alignment seggregation alignment scratch store company store company informationpreparing prototype phase informationpreparing reports prototype phase blockbuster sense ownership work responsibilities dutieslead end results targeting database solutions management team founder advanced excelexperience crm advanced excelexperience term objectives data term objectives mis specialist objectives stages product objectives stages embed business intelligence embed business users stakeholders develop users stakeholders years experiencejob summaryan role involves data type yearexperience data solutions store company market enabling market enabling business lifetime chance make lifetime chance involves data extraction involves data leader responsibilities team founder highly build rocket ship leader responsibilities dutieslead supporting decision makinganalyse solutions store data extraction seggregation specialist join tools js data build rocket scratch approach data reports provide business framework build rocket framework build winning management team winning management phase blockbuster phase blockbuster results business analytics leader analytics structure analytics structure growing blockbuster results targeting blockbuster results blockbuster institute advanced scratch approach mysql advanced\n",
      "\n",
      "Cluster 4: big data big mix technology business bringing mix technology bringing mix big data machine come ip pre come ip years experiencetredence years experiencetredence analytics mix technology mining bi using execution data management advanced existing technology data visualization data management range data visualization assets come assets come ip ip pre karnataka years experiencetredence using execution model machine learning bringing clients end effectively using effectively using execution clients existing technology clients existing experiencetredence experiencetredence analytics experiencetredence analytics services clients end engagement data mining bi technology data assets advanced analytics big model leveraging model leveraging clients analytics create analytics create box execution model execution model leveraging business analytics create ip pre built leveraging clients learning bringing learning bringing mix capabilities range data transitioned clients end create box solutions cost effectively using transitioned clients transitioned capabilities range cost effectively data assets come leveraging clients existing create box clients existing technology technology data box solutions range data end engagement management advanced analytics analytics services management advanced data assets analytics big data analytics big pre built ip data machine learning data machine data visualization data visualization data technology business analytics box bringing built data management come advanced analytics engagement tredence companies silicon valley companies silicon technology business pre built analytics company capabilities company capabilities range analytics solutions data solutions transitioned box solutions transitioned bi big end engagement cost bi big data analytics services solutions built analytics solutions services solutions company solutions transitioned clients\n",
      "\n",
      "Cluster 5: seo marketing search time job engine optimization social media marketing search engine optimization search engine media marketing social media promotion paid digital marketing social learn engine jan content brand hyderabad digital designing media include day optimization apply meetings agencies vendors interns presented support logo designing meetings agencies brand grow potential interns presented applyonly candidates apply applyonly candidates logo designing pay brand grow search amazon projectsparticipate email sms marketing field preferredperkscertificate types time temporarysalary pay click management strengthen presence leave encashmentphone internet leave encashmentphone help brand grow help brand pay click vendors partnersassist vendors partnersassist timelines candidates apply time agency based hyderabad applyonly agency based india strengthen india strengthen presence agencies vendors partnersassist developing promotion email field preferredperkscertificate letter services hyderabad include services hyderabad projectsparticipate support marketing agency support audit process developing promotion partnersassist timelines intelligence researchattend amazon projectsparticipate grow potential process jobs internships amazon projectsparticipate summer process jobs th jan duration audit process partnersassist partnersassist timelines search marketing agency based audit process jobs temporarysalary monthbenefits paid click management temporarysalary monthbenefits click management ppc learningconduct intelligence researchattend temporarysalary encashmentphone internet reimbursementindustry encashmentphone internet marketing search intelligence researchattend meetings search amazon summer jan th jan dress code job hyderabad india strengthen projectsparticipate summer marketing services hyderabad based hyderabad india skill required social skill required campaigns business summer project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 6: innovation care insight trade consumer analytics consumer receive training media trade retailers data analytics business analysts receive predictive valued today ba based data brands environment analysts diversity media core training life day initiatives ideas market data science value ability query analyze trade efficiency ability query data mining ai opportunities brands trade cost inclusion valued algorithms conclusions algorithms conclusions business trade cost saving channels retailers trade efficiency media capacity crafting tomorrow capacity crafting growth opportunities brands channels retailers levels crafting mentorship collaborate mentorship collaborate passionate trade channels retailers crafting tomorrow innovation innovation passion hardworking crafting tomorrow innovation passion inclusion valued included sciencespreferred trade channels business analysts deliver data market share environment place work heard inspired heard inspired offer environment place business analysts ba soap company today soap company business insightskey responsibilities business insightskey models based models based analysis projects skills required exciting assignments exciting assignments initiatives analysts ba analysts ba authority good mix opportunities accelerate good mix delivering skills required ability passion hardworking passion hardworking teams innovation projects innovation projects seek languages python sqlvisualization flourish flourish receive flourish receive training analysts deliver business work identify growth work identify projects seek existing opportunities accelerate business opportunities brands country ways spanned centuries develop predictive models balance just seek existing business seek existing based analytics consumer\n",
      "\n",
      "Cluster 7: hts language project systems sap minimum statistical business analysis planning project consulting analysis statistical expected anaplan english hts sap skills english language sap project english language language skills english skills english required yearsmandatory required yearsmandatory skills experience required yearsmandatory responsibilities minimum yearsmandatory sas statistics minimum experience required responsibilities minimum experience yearsmandatory skills mandatory skills required job minimum experience language skills required job analytics consulting skills language skills business practice experience required statistical analysis business analysis statistical business analytics sas analysis statistical analysis analytics sas analytics sas statistics skills language skills aero hts sap project sap project systems client facing skills analytics consulting minimum work minimum work experience business analyst skills analytics description key key skills required description key skills skills business analysis modelingdesirable skills statistical modelingdesirable skills sas statistics statistical statistical modelingdesirable modelingdesirable statistics statistical modelingdesirable statistics statistical years skills analysis business analytics karnataka years skills modelingdesirable skills language key skills dw bi experience yearsroles responsibilities work experience yearsroles yearsroles responsibilities minimum yearsroles responsibilities experience yearsroles yearsroles skills required mandatory business olap client statistical analysis business mandatory business analyst facing dw qlikview qlik domain analysis business sas description financial expected client requirements teams terms analysis decomposition analysis information expected information expected client tools minimum tools minimum work analysis decomposition\n",
      "\n",
      "Cluster 8: fargo wells wells fargo apptio frra risk reporting highly advanced windows firmwide capital capital stress strategy analytics stress testing capital stress testing stress controllers eads semi supervised vs strategy processes enterprise egs mac windows server performance highly finance os aster performance indicators driven frra team reporting capital stress stress testing including reporting capital firm reporting testing including indicators analytics team talent bank implementation banking alteryx business services supervise bengaluru provides partner multi tasking tasking testing intelligence data business intelligence data windows server enterprise server enterprise following configuration management maintain driven decisions data driven decisions maintaining employment microsoft issues timely begins track credit kpis server international careers international careers website accuracy define careers website services egs customers realise focus advanced help customers realise years working capabilities kpis manner thorough business thorough stakeholders portfolio controllers develop weekly monthly stakeholders portfolio manner conduct develop weekly manner conduct thorough reports presentation reports presentation decks presentation decks team hr operations team interface team interface stakeholders\n",
      "\n",
      "Cluster 9: ensure requirements capturing met ensure gathering software development business requirements document practices solution work experience lead client software development requirements documents line ensure users translated specifications document improve document improve workflow role impact line practices communication brd use case documented pre agreed requirements met ensure documented pre problem solving develop line ensure business gathering session improve workflow basically gathering session demonstrating solution work appreciation work capturing scope case model solution work relationship client stakeholders business requirements clearly relationship client job descriptionsutherland job descriptionsutherland seeking business requirements document gathering requirements developing brd use individuals looking succeed work leads client requirement gathering wire frame process client requirement improve workflow aligned apply practices specifications ensure requirements wire frame technology platforms including aligned apply work appreciation understanding developing business requirements succeed work users translated leadership people management leadership people model use practices communication problem session work capturing session demonstrating session demonstrating solution model use cases individuals looking build case model use understands respects pre agreed formats player passion strengthen relationships engage capturing requirements looking build fulfilling looking build met proposed solution met proposed work organizing meetings capturing requirements turning work organizing met ensure requirements met ensure practices work organizing practices work translated specifications translated specifications ensure develop detailed project requirements turning srs requirements turning manage relationship client develop detailed use case model seeking goal seeking goal oriented understanding technology platforms capturing scope work experience gathering capturing scope change player passion understands specs wire frame\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\",'\\n')\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tf_idf.get_feature_names()\n",
    "for i in range(10):\n",
    "    top_ten_words = [terms[ind] for ind in order_centroids[i, :100]]\n",
    "    print(\"Cluster {}: {}{}\".format(i, ' '.join(top_ten_words),'\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key word extraction using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_title', 'job_description', 'clean_text', 'text_stop'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'token_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'token_pos'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-833e83ed1d29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the list of list format of the custom corpus for gensim modeling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# show the example of list of list format of the custom corpus for gensim mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'token_pos'"
     ]
    }
   ],
   "source": [
    "# Create the list of list format of the custom corpus for gensim modeling \n",
    "sent = [row.split(' ') for row in df['token_pos'].dropna()]\n",
    "# show the example of list of list format of the custom corpus for gensim mode\n",
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the list and add the size of all internal lists \n",
    "count = 0\n",
    "for listElem in sent:\n",
    "    count += len(listElem)                    \n",
    " \n",
    "print('Total Number of elements : ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sent, min_count=1,size= 220,workers=5, window =5, sg = 1)\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "result = pca.fit_transform(X)\n",
    "tmp = pd.DataFrame(result, index=model.wv.vocab, columns=['x', 'y','z'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the word2vec term matrix into a datafarame\n",
    "ordered_vocab = [(term, voc.index, voc.count) for term, voc in model.wv.vocab.items()]\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda k: k[2])\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "word_vectors = pd.DataFrame(model.wv.syn0[term_indices, :], index=ordered_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_skills=['Excel', 'SPSS', 'SAS', 'SQL', 'Oracle', 'PowerBI', 'Qlik', 'Python', 'R', 'Tableau', \n",
    "             'Qualtrics', 'SAP', 'Pentaho', 'Shiny','Pandas', 'Alteryx', 'Oracle', 'Cognos', 'Hive',\n",
    "              'Pyspark','HDFS','MapReduce','Hadoop','ETL','KPI']\n",
    "\n",
    "concept_skills=['forecasting','MachineLearning', 'DataMining', 'DataScience', 'metrics', 'statistical', \n",
    "                'predictive', 'DataAnalysis', 'BusinessIntelligence',  'visualisation', 'dashboards', \n",
    "                'survey', 'database','regression','Reporting_Analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[]\n",
    "concepts=[]\n",
    "\n",
    "for item in hard_skills:\n",
    "    tools.append(model.wv.most_similar(positive=[item],topn=30))\n",
    "    \n",
    "for item in concept_skills:\n",
    "    concepts.append(model.wv.most_similar(positive=[item],topn=30))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_name= [i[0] for item in tools for i in item ]\n",
    "print('No of tools',len(tools_name))\n",
    "\n",
    "tools_corr= [i[1] for item in tools for i in item ]\n",
    "print('No of tools',len(tools_name))\n",
    "\n",
    "concept_name= [i[0] for item in concepts for i in item ]\n",
    "print('No of tools',len(concept_name))\n",
    "\n",
    "concept_corr= [i[1] for item in concepts for i in item ]\n",
    "print('No of tools',len(concept_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tools_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-7603b340cedf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtools_tuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtools_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtools_corr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_tuples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tools'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Corr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconcept_tuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconcept_corr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tools_name' is not defined"
     ]
    }
   ],
   "source": [
    "tools_tuples = list(zip(tools_name,tools_corr))\n",
    "tools=pd.DataFrame(data_tuples, columns=['Tools','Corr'])\n",
    "print(tools.shape)\n",
    "\n",
    "concept_tuples = list(zip(concept_name,concept_corr))\n",
    "concepts=pd.DataFrame(concept_tuples, columns=['concepts','Corr'])\n",
    "print(concepts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.to_csv('tools.csv',header=True)\n",
    "concepts.to_csv('concepts.csv',header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-532c325dcd08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#to show a df simply use the below:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mqgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tools' is not defined"
     ]
    }
   ],
   "source": [
    "#to show a df simply use the below:\n",
    "qgrid.show_grid(tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
